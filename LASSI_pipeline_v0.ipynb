{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d11fe58-6e4f-413a-b326-20d9d478ef89",
   "metadata": {},
   "source": [
    "# LLM4Code **Translation** + HPC \n",
    "\n",
    "***LMStudio API*** or ***OLLAMA API*** Jupyter Notebook version\n",
    "\n",
    "Code translation source and target scripts from:\n",
    "https://github.com/zjin-lcf/HeCBench/tree/master/src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1a0d2a-0fd9-43a5-a60c-7ecfcb9ea745",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efdff123-043e-48a8-a87b-e15a8acb4b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "import shlex  # for splitting command line arguments\n",
    "from openai import OpenAI\n",
    "import ollama  # !pip install ollama\n",
    "from prompt_dictionary import PromptDictionary\n",
    "\n",
    "# Used in similarity metrics\n",
    "import difflib\n",
    "import tokenize  # apparently only defined for Python code\n",
    "import tiktoken\n",
    "from io import BytesIO\n",
    "\n",
    "# Enable the autoreload extension\n",
    "%load_ext autoreload\n",
    "\n",
    "# Reload all modules imported with %aimport every time before executing subsequent code.\n",
    "%autoreload 1\n",
    "    \n",
    "# We need to reload the prompt dictionary because it is under development\n",
    "%aimport prompt_dictionary\n",
    "    \n",
    "# Text formatting options\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19781fda-4b90-4b50-a002-6f06dea85754",
   "metadata": {},
   "source": [
    "## LLM Configuration Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb6075c2-a0c9-484b-acda-e8b3729bbad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Local hosted LLM via LMStudio.ai API, GGUF quantized models\n",
    "\"\"\"\n",
    "def lmstudio_llm(model, system_prompt, content_prompt):\n",
    "    client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "    \n",
    "    chat_response = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": content_prompt}\n",
    "      ],\n",
    "      temperature=0.2,\n",
    "      stream=False\n",
    "    )\n",
    "\n",
    "    response_text = chat_response.choices[0].message.content\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13276ffe-b8e9-4eb5-a255-bd7f5ab58f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Local hosted LLM via OLLAMA API, GGUF quantized models\n",
    "\n",
    "Requirement: ollama serve [from server command line]\n",
    "Requirement: ollama.pull('modelname') [from Python script to pre-load selected model at the server]\n",
    "\n",
    "ollama.list() [preview what is available to use on the server]\n",
    "\n",
    "Example: model='codestral:latest'\n",
    "\"\"\"\n",
    "def ollama_llm(model, num_ctx, system_prompt, content_prompt):\n",
    "    # Ollama serve must already be running on a port on this server \n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": content_prompt}\n",
    "        ],\n",
    "        stream=False,\n",
    "        options=dict(\n",
    "            temperature=0.2,\n",
    "            top_p=0.9,\n",
    "            num_ctx=num_ctx\n",
    "        )\n",
    "    )\n",
    "\n",
    "    response_text = response['message']['content']\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6da6bf72-9553-4b17-af94-45795b7080bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unload_ollama_model(model):\n",
    "    print(\"\\nUnloading Ollama server...\")\n",
    "    data_curl = {\n",
    "        \"model\": model,\n",
    "        \"keep_alive\": 0\n",
    "    }\n",
    "    data_curl_json = json.dumps(data_curl)\n",
    "    curl_command_unload_LLM = [\n",
    "        'curl',\n",
    "        'http://localhost:11434/api/generate',\n",
    "        '-d',\n",
    "        data_curl_json\n",
    "    ]\n",
    "    result_unload_LLM = subprocess.run(curl_command_unload_LLM, capture_output=True, text=True)\n",
    "    print(\"\\nUnload LLM:\\n\", result_unload_LLM.stdout)\n",
    "    print(result_unload_LLM.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d697f2-40a6-47c4-8afb-522d537178a2",
   "metadata": {},
   "source": [
    "## Pipeline Utility Method Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f75c11-0039-41f1-a5de-3a9097d3bfb1",
   "metadata": {},
   "source": [
    "### LLM Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c92e788e-07de-4d64-bb64-a9ae81930d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_knowledge_llm(llm_source, model, num_ctx, system_prompt, input_context, target_lang, display_response):\n",
    "\n",
    "    content_prompt = \"\"\"\n",
    "    Before translating the code, please provide a summary description the following background knowledge and context on the programming language.\n",
    "    \"\"\"\n",
    "\n",
    "    if target_lang == \"OMP\":\n",
    "        content_prompt = content_prompt + \"\"\" \n",
    "        Specifically, describe how to implement the omp pragma directive 'target teams' for distributing for loops on a GPU.\n",
    "        \"\"\"\n",
    "        \n",
    "    content_prompt = content_prompt + \"\"\" \n",
    "    Only respond with your summary description and avoid repeating the code: \n",
    "    \"\"\"\n",
    "\n",
    "    content_prompt = content_prompt + input_context\n",
    "    \n",
    "    if llm_source == \"argo\":\n",
    "        response_text = argo_llm(model, system_prompt, content_prompt)\n",
    "    elif llm_source == \"lmstudio\":\n",
    "        response_text = lmstudio_llm(model, system_prompt, content_prompt)\n",
    "    elif llm_source == \"ollama\":\n",
    "        response_text = ollama_llm(model, num_ctx, system_prompt, content_prompt)\n",
    "\n",
    "    print(\"\\n\\n----- \" + color.BLUE + \"CONTEXT KNOWELEDGE SUMMARY\" + color.END + \" -----\")\n",
    "    print(\"(generated by the LLM)\\n\")\n",
    "    if display_response:\n",
    "        display(Markdown(response_text))\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68f5b463-c657-4a3c-aaf8-1dd0b438d54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_description_llm(llm_source, model, num_ctx, system_prompt, input_code, display_response):\n",
    "\n",
    "    content_prompt = \"\"\"\n",
    "    Before translating the code, please provide a summary description of the following code. \n",
    "    Only respond with your summary description and avoid repeating the code: \n",
    "    \"\"\"\n",
    "    \n",
    "    content_prompt = content_prompt + input_code\n",
    "    \n",
    "    if llm_source == \"argo\":\n",
    "        response_text = argo_llm(model, system_prompt, content_prompt)\n",
    "    elif llm_source == \"lmstudio\":\n",
    "        response_text = lmstudio_llm(model, system_prompt, content_prompt)\n",
    "    elif llm_source == \"ollama\":\n",
    "        response_text = ollama_llm(model, num_ctx, system_prompt, content_prompt)\n",
    "\n",
    "    print(\"\\n\\n----- \" + color.BLUE + \"CODE DESCRIPTION\" + color.END + \" -----\")\n",
    "    print(\"(generated by the LLM)\\n\")\n",
    "    if display_response:\n",
    "        display(Markdown(response_text))\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00ddfed0-af7a-4236-8d82-b4b816784e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_generator_llm(llm_source, model, num_ctx, system_prompt, content_prompt, display_response):\n",
    "\n",
    "    if llm_source == \"argo\":\n",
    "        response_text = argo_llm(model, system_prompt, content_prompt)\n",
    "    elif llm_source == \"lmstudio\":\n",
    "        response_text = lmstudio_llm(model, system_prompt, content_prompt)\n",
    "    elif llm_source == \"ollama\":\n",
    "        response_text = ollama_llm(model, num_ctx, system_prompt, content_prompt)\n",
    "        \n",
    "    print(\"\\n\\n----- \" + color.BLUE + \"GENERATED CODE\" + color.END + \" -----\\n\\n\")\n",
    "    if display_response:\n",
    "        display(Markdown(response_text))\n",
    "\n",
    "    # Extract only the code block - \n",
    "    # Search for the ``` pattern in the text\n",
    "    #   Regular expression pattern to match text between triple backticks \n",
    "    #   that represent code blocks within generated responses from the LLM.\n",
    "    pattern = r\"```(.*?)```\"\n",
    "    match = re.search(pattern, response_text, re.DOTALL)\n",
    "    # print(\"RESPONSE TEXT -- \", response_text)\n",
    "    # print(\"MATCH -- \", match)\n",
    "    if match:\n",
    "        extracted_text = match.group(1)\n",
    "    else:\n",
    "        extracted_text = \"\"\n",
    "    \n",
    "    return response_text, extracted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5202bf2-de4e-483c-93a6-193f28f12753",
   "metadata": {},
   "source": [
    "### Code Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0fe91ba-6894-43b3-a964-654266dcf704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_code(source_file, lang, code_compiler, code_compiler_kwds):\n",
    "       \n",
    "    # Split the compile keywords into a list of arguments\n",
    "    compile_kwds_list = shlex.split(code_compiler_kwds)\n",
    "\n",
    "    if lang == \"OMP\":\n",
    "        # The desired name of the compiled program\n",
    "        output_file = source_file.split('.cpp')[0]\n",
    "        # Construct the compilation command\n",
    "        compile_command = [code_compiler] + compile_kwds_list + [source_file, '-o', output_file]\n",
    "    \n",
    "    elif lang == \"CUDA\":\n",
    "        # The desired name of the compiled program\n",
    "        output_file = source_file.split('.cu')[0]\n",
    "        # Construct the compilation command\n",
    "        compile_command = [code_compiler] + compile_kwds_list + [source_file, '-o', output_file]\n",
    "        \n",
    "    print(\"Compile command: \" + str(compile_command))\n",
    "    \n",
    "    # Execute the compilation command\n",
    "    result = subprocess.run(compile_command, capture_output=True, text=True)\n",
    "\n",
    "    # Check if the compilation was successful\n",
    "    return_result = \"\"\n",
    "    if result.returncode == 0:\n",
    "        print(\"\\n\" + color.BOLD + \"Compilation of code successful!\" + color.END)\n",
    "    else:\n",
    "        # If there was an error, print the error message\n",
    "        return_result = result.stderr\n",
    "        print(\"\\n\" + color.BOLD + color.RED + \"***** Compilation failed.\" + color.END)\n",
    "        print(\" Error message: \")\n",
    "        print(return_result)\n",
    "        \n",
    "    return return_result, output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "976745b6-f871-4500-9527-950a7f30c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_code(executable_file, exe_input_parameters):\n",
    "\n",
    "    # Capture current directory to switch back after code execution\n",
    "    initial_dir = os.getcwd()\n",
    "\n",
    "    # Extract the compiled program to execute\n",
    "    directory_path = os.path.dirname(executable_file)\n",
    "    executable = os.path.basename(executable_file)\n",
    "\n",
    "    # Change the working directory to the subdirectory of the executable\n",
    "    os.chdir(directory_path)\n",
    "\n",
    "    # Ensure the binary has execute permissions\n",
    "    subprocess.run(['chmod', '+x', executable])\n",
    "\n",
    "    # Build execute command\n",
    "    execute_command = ['./' + executable] + exe_input_parameters\n",
    "    print(\"Execute command: \" + str(execute_command) + \"\\n\")\n",
    "\n",
    "    # Execute the binary object with input parameters, if provided\n",
    "     \n",
    "    # Ensure the file is open and available before executing\n",
    "    if os.path.exists(executable):\n",
    "        try:\n",
    "            # Open the file to ensure it is accessible\n",
    "            with open(executable, 'rb') as code_file:\n",
    "                # Start the process\n",
    "                process = subprocess.Popen(execute_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "                stdout_lines = []\n",
    "                stderr_lines = []\n",
    "                exception_error = \"\"\n",
    "                \n",
    "                # Print the output in real-time\n",
    "                try:\n",
    "                    # Read stdout in a separate thread or in a non-blocking way\n",
    "                    while True:\n",
    "                        line = process.stdout.readline()\n",
    "                        if not line:\n",
    "                            break\n",
    "                        print(line, end='')  # Print each line as it is received\n",
    "                        stdout_lines.append(line)\n",
    "                    process.stdout.close()\n",
    "            \n",
    "                    # Read stderr in a separate thread or in a non-blocking way\n",
    "                    while True:\n",
    "                        line = process.stderr.readline()\n",
    "                        if not line:\n",
    "                            break\n",
    "                        stderr_lines.append(line)\n",
    "                    process.stderr.close()\n",
    "                \n",
    "                    # Wait for the process to complete and get the return code\n",
    "                    return_code = process.wait()\n",
    "                    \n",
    "                    if return_code != 0:\n",
    "                        error_output = ''.join(stderr_lines)\n",
    "                        print(\"Program Execution Error:\", error_output)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"An error occurred while executing the program: \", e)\n",
    "                    exception_error = str(e)\n",
    "        except IOError as e:\n",
    "            print(f\"Pipeline error | An I/O error occurred: {e.strerror}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Pipeline error | An error occurred: {e}\")\n",
    "    else:\n",
    "        print(f\"Pipeline error | The file {executable} does not exist.\")\n",
    "    \n",
    "    \n",
    "    # Check if the program ran successfully\n",
    "    return_result = \"0\"  # default OK\n",
    "    if return_code == 0:\n",
    "        print(\"\\n\" + color.BOLD + \"Program executed successfully!\" + color.END)\n",
    "    else:\n",
    "        # If there was an error during execution, print the error message\n",
    "        return_result = \"Program exited with return code: \" + str(return_code) + \" \"\n",
    "        if return_code == -11:\n",
    "            return_result += \"Segmentation fault detected.\"\n",
    "        stderr_output = ''.join(stderr_lines)\n",
    "        # stdout_output = ''.join(stdout_lines)\n",
    "        # if stdout_output:\n",
    "        #    return_result += \"\\nStandard Output: \" + stdout_output\n",
    "        if exception_error != \"\":\n",
    "            return_result += \"\\nException Error: \" + exception_error\n",
    "        if stderr_output:\n",
    "            return_result += \"\\nStandard Error: \" + stderr_output\n",
    "        print(\"\\n\" + color.BOLD + color.RED + \"***** Execution failed.\" + color.END)\n",
    "        print(\"Error message: \")\n",
    "        print(return_result)\n",
    "    \n",
    "    os.chdir(initial_dir)\n",
    "\n",
    "    stdout_output = ''.join(stdout_lines)\n",
    "    \n",
    "    return return_result, stdout_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a949bc43-3e33-4e51-815d-2ef27c350835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_count(doc_text):\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    encoding = tiktoken.Encoding(\n",
    "        name=\"gpt-35-turbo\",\n",
    "        pat_str=tokenizer._pat_str,\n",
    "        mergeable_ranks=tokenizer._mergeable_ranks,\n",
    "        special_tokens={\n",
    "            **tokenizer._special_tokens,\n",
    "            \"<|im_start|>\": 100264,\n",
    "            \"<|im_end|>\": 100265,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    tokens_doc = encoding.encode(\n",
    "        doc_text, allowed_special={\"<|im_start|>\", \"<|im_end|>\"}\n",
    "    )\n",
    "\n",
    "    return len(tokens_doc)\n",
    "    \n",
    "def tokenize_by_tiktoken(code):\n",
    "    # Get an encoding (e.g., \"cl100k_base\")\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    # Encode and decode a sample text\n",
    "    encoded_tokens = enc.encode(code)\n",
    "    decoded_code = enc.decode(encoded_tokens)\n",
    "\n",
    "    # Verify that the decoding works correctly\n",
    "    assert decoded_code == code\n",
    "\n",
    "    return encoded_tokens\n",
    "\n",
    "def tokenize_by_tokenize(code):\n",
    "    # Includes an error handling to \"skip\" over bad token conversions\n",
    "    tokens = []\n",
    "    try:\n",
    "        readline = BytesIO(code.encode('utf-8')).readline\n",
    "        token_gen = tokenize.tokenize(readline)\n",
    "        while True:\n",
    "            try:\n",
    "                token = next(token_gen)\n",
    "                tokens.append(token)\n",
    "            except IndentationError as e:\n",
    "                print(f\"Indentation error in the code: {e}\")\n",
    "                continue\n",
    "            except StopIteration:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def tokenize_code(code, method=\"tokenize\"):\n",
    "\n",
    "    if method == \"tokenize\":\n",
    "        tokens = tokenize_by_tokenize(code)\n",
    "    elif method == \"tiktoken\":\n",
    "        tokens = tokenize_by_tiktoken(code)\n",
    "    \n",
    "    return tokens\n",
    "    \n",
    "def token_similarity(code1, code2, method):\n",
    "    tokens1 = tokenize_code(code1, method)\n",
    "    tokens2 = tokenize_code(code2, method)\n",
    "    sequence_matcher = difflib.SequenceMatcher(None, tokens1, tokens2)\n",
    "    return sequence_matcher.ratio()\n",
    "\n",
    "def find_line_in_lines(line, lines):\n",
    "    \"\"\"Helper function to find a line in a list of lines.\"\"\"\n",
    "    for i, l in enumerate(lines):\n",
    "        if line.strip() == l.strip():\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def compare_lines_with_reordering(code1, code2):\n",
    "    \"\"\"Compare lines of code1 with lines of code2 allowing reordering.\"\"\"\n",
    "    lines1 = code1.splitlines()\n",
    "    lines2 = code2.splitlines()\n",
    "    \n",
    "    matched_lines = 0\n",
    "    for line1 in lines1:\n",
    "        index = find_line_in_lines(line1, lines2)\n",
    "        if index != -1:\n",
    "            matched_lines += 1\n",
    "            lines2.pop(index)  # Remove the matched line to prevent duplicate matching\n",
    "    \n",
    "    total_lines = max(len(lines1), len(code2.splitlines()))\n",
    "    similarity_ratio = matched_lines / total_lines if total_lines > 0 else 0\n",
    "    return similarity_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfbfced-994b-49cd-be05-282be5ae007b",
   "metadata": {},
   "source": [
    "### The Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdb5b82f-b135-4226-92ff-056d69e17903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_code_llmgeneration_pipeline(this_llm_systemcode_prompt,\n",
    "                                     this_llm_prompt,\n",
    "                                     llm_source, \n",
    "                                     model,\n",
    "                                     num_ctx,\n",
    "                                     # arrayormatrix_size_range,\n",
    "                                     # thread_count_range,\n",
    "                                     use_context,\n",
    "                                     input_context,\n",
    "                                     input_code,\n",
    "                                     target_code,\n",
    "                                     translated_code_extension,\n",
    "                                     target_lang,\n",
    "                                     code_compiler,\n",
    "                                     code_compiler_kwds,\n",
    "                                     directory,\n",
    "                                     file_name,\n",
    "                                     target_file_path,\n",
    "                                     display_response,\n",
    "                                     test_with_error,\n",
    "                                     generate_test_case,\n",
    "                                     yes_execute,\n",
    "                                     exe_input_parameters,\n",
    "                                     meta_data_file\n",
    "                                    ):\n",
    "    \n",
    "    # Test compile and execute the target souurce code to validate compiler and execution environment\n",
    "    print(\"\\n\\n----- \" + color.BLUE + \"TARGET SOURCE CODE: Test compile and execute\" + color.END + \" -----\")\n",
    "    print(\"\\nTest compile original target code: \" + str(target_file_path))\n",
    "    compile_result, execute_file = compile_code(target_file_path, target_lang, code_compiler, code_compiler_kwds)\n",
    "    if compile_result != \"\":\n",
    "        return False\n",
    "    if yes_execute:\n",
    "        print(\"\\nTest executing the compiled original target code: \" + str(execute_file))\n",
    "        exe_file_path = execute_file  # directory + \"/\" + execute_file\n",
    "        execute_result, exe_output_source = execute_code(exe_file_path, exe_input_parameters)\n",
    "        if execute_result != \"0\":\n",
    "            return False\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    compile_result = \"start\"\n",
    "    content = \"\"\n",
    "    execute_result = \"\"\n",
    "    error_message = \"\"\n",
    "    error_source = \"\"  # \"compiled\" or \"executed\"\n",
    "    code_block = \"\"\n",
    "    self_correction_counter = -1\n",
    "\n",
    "    general_system_prompt = prompts.get_system_prompt(\"general_system\")\n",
    "    \n",
    "    # Augmented prompting to handle processing of provided knowledge context\n",
    "    if use_context:\n",
    "        content = \"[BEGIN CONTEXT KNOWLEDGE] \" + input_context + \" [END CONTEXT KNOWLEDGE]\\n \"\n",
    "    \n",
    "        llm_code_knowledge_summary_response = code_knowledge_llm(llm_source, model, num_ctx, general_system_prompt, input_context, target_lang, display_response)\n",
    "        \n",
    "        content = content + \"[BEGIN CONTEXT SUMMARY] \" + llm_code_knowledge_summary_response + \" [END CONTEXT SUMMARY]\\n \"\n",
    "        \n",
    "        content = content + \"Using the above background context with your summarization, \"\n",
    "\n",
    "        llm_code_description_response = code_description_llm(llm_source, model, num_ctx, general_system_prompt, input_code, display_response)\n",
    "    \n",
    "        content = content + \"Think carefully before developing the following code that you describe as: \"\n",
    "\n",
    "        content = content + llm_code_description_response\n",
    "    else:\n",
    "        content = general_system_prompt + \"Think carefully before developing the following request: \"\n",
    "\n",
    "    # Save the original context prompt for re-use during error correction.\n",
    "    original_context_content_prompt = content\n",
    "    original_prompt_tokens = token_count(original_context_content_prompt)\n",
    "    \n",
    "    content = content + \" Now, \" + this_llm_prompt\n",
    "    content = content + \" \" + input_code\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # INITIATE LLM CODE GENERATION ITERATIVE PIPELINE\n",
    "    while compile_result != \"\":\n",
    "        self_correction_counter = self_correction_counter + 1\n",
    "        \n",
    "        if compile_result == \"start\":\n",
    "            # CALL THE LLM\n",
    "            content = re.sub(' +', ' ', content)\n",
    "            print(\"\\n\\n----- \" + color.BOLD + \"PROMPT\" + color.END + \" (initial, token count: \" + str(original_prompt_tokens) + \"): \\n\" + content + \"\\n\")\n",
    "            code_response, code_block = code_generator_llm(llm_source, model, num_ctx, this_llm_systemcode_prompt, content, display_response)\n",
    "        else:\n",
    "            # CALL THE LLM WITH ERROR CORRECTION\n",
    "            print(\"\\n\\n----- \" + color.BOLD + color.RED + \"SELF-CORRECTION: \" + str(self_correction_counter) + color.END + \" -----\")\n",
    "            if compile_result == \"execute_error\":\n",
    "                error_correction_prompt_intro = \"\\n -- The above code was executed after a successul compile with \" + code_compiler + \" \" + code_compiler_kwds + \" and produced the following execution error: \"\n",
    "                error_message = execute_result\n",
    "            else:  # Must be a compiler error\n",
    "                # NOTE: Maybe do not provide the original code in the other language when trying to fix code in the target language.\n",
    "                error_correction_prompt_intro = \"\\n -- The above code was compiled with \" + code_compiler + \" \" + code_compiler_kwds + \" and produced the following compile error: \"\n",
    "                error_message = compile_result\n",
    "            \n",
    "            # error_correction_prompt_outro = \" Eliminate this specific error by re-factoring the above code with an appropriate fix while also using the following background context: \" + original_context_content_prompt\n",
    "            # error_prompt = code_block + error_correction_prompt_intro + error_message + error_correction_prompt_outro\n",
    "            # error_correction_prompt_outro = \" Eliminate this specific error by re-factoring the above code with an appropriate fix.\"\n",
    "            error_correction_prompt_outro = \" Re-factor the above code with a fix to eliminate the stated error. \"\n",
    "            error_prompt = code_block + error_correction_prompt_intro + error_message + error_correction_prompt_outro\n",
    "            error_prompt_tokens = token_count(error_prompt)\n",
    "            print(\"\\n\\n----- \" + color.BOLD + color.RED + \"ERROR SELF-CORRECTION\" + color.END + \" -----\")\n",
    "            print(\"\\nPROMPT (error correction, token count: \" + str(error_prompt_tokens) + \"): \" + error_prompt + \"\\n\")\n",
    "            error_prompt = re.sub('\\n', '', error_prompt)\n",
    "            code_response, code_block = code_generator_llm(llm_source, model, num_ctx, this_llm_systemcode_prompt, error_prompt, display_response)\n",
    "\n",
    "        if code_block.startswith('c++') or code_block.startswith('cpp'):\n",
    "            code_block = code_block[3:]\n",
    "        if code_block.startswith('c'):\n",
    "            code_block = code_block[1:]    \n",
    "    \n",
    "        # SAVE THE GENERATIVE CODE -- before compiling\n",
    "        file_path = directory + \"/\" + file_name + \".\" + translated_code_extension\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(code_block)\n",
    "        file.close()\n",
    "    \n",
    "        # Compile the generated code\n",
    "        compile_result, execute_file = compile_code(file_path, target_lang, code_compiler, code_compiler_kwds)\n",
    "\n",
    "        print(\"\\nExecute file: \" + str(execute_file))\n",
    "\n",
    "        if yes_execute and compile_result == \"\" and self_correction_counter <= 7:\n",
    "            # Unload any previously loaded LLMs in the Ollama server \n",
    "            # (clear out the memory)\n",
    "            if llm_source == \"ollama\":\n",
    "                unload_ollama_model(model)\n",
    "            \n",
    "            exe_file_path = execute_file  # directory + \"/\" + execute_file\n",
    "            execute_result, exe_output = execute_code(exe_file_path, exe_input_parameters,)\n",
    "            if execute_result != \"0\":\n",
    "                compile_result = \"execute_error\"  # just a flag\n",
    "                \n",
    "    # end of while pipeline loop\n",
    "\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%m-%d-%Y_t%H_%M_%S\")\n",
    "    \n",
    "    print(\"\\n-----\" + color.GREEN + \"CODE GENERATION COMPLETE\" + color.END + \"-----\\n\")\n",
    "    end_time_minutes = round(end_time/60, 2)\n",
    "    print(\"CODE GENERATION TIME: \" + str(end_time_minutes) + \" minutes.\\n\")\n",
    "    print(\"Self-correction count: \" + str(self_correction_counter) + \"\\n\")\n",
    "\n",
    "    # MEASURE SIMILARITY BETWEEN GENERATED TRANSLATION CODE AND ORIGINAL TARGET CODE\n",
    "    similarity_tokens = token_similarity(target_code, code_block, \"tokenize\")\n",
    "    print(f\"Token similarity (tokenize): {similarity_tokens:.2f}\")\n",
    "\n",
    "    similarity_tokens_tiktoken = token_similarity(target_code, code_block, \"tiktoken\")\n",
    "    print(f\"Token similarity (tiktoken): {similarity_tokens_tiktoken:.2f}\")\n",
    "    \n",
    "    similarity_lines = compare_lines_with_reordering(target_code, code_block)\n",
    "    print(f\"Line-based similarity with reordering: {similarity_lines:.2f}\")\n",
    "\n",
    "    meta_data_file += \"\\n\\n\"\n",
    "    meta_data_file += \"----- METRICS -----\"\n",
    "    meta_data_file += \"\\n\"\n",
    "    meta_data_file += \"Experiment date-time: \" + str(dt_string)\n",
    "    meta_data_file += \"\\n\"\n",
    "    meta_data_file += \"Time (sec): \" + str(round(end_time, 2))\n",
    "    meta_data_file += \"\\n\"\n",
    "    meta_data_file += f\"Token similarity (tokenize): {similarity_tokens:.2f}\"\n",
    "    meta_data_file += \"\\n\"\n",
    "    meta_data_file += f\"Token similarity (tiktoken): {similarity_tokens_tiktoken:.2f}\"\n",
    "    meta_data_file += \"\\n\"\n",
    "    meta_data_file += f\"Line-based similarity with reordering: {similarity_lines:.2f}\"\n",
    "    meta_data_file += \"\\n\"\n",
    "    meta_data_file += \"Self-correction iteration count: \" + str(self_correction_counter)\n",
    "    if yes_execute:\n",
    "        meta_data_file += \"\\n\\n\\n----- SOURCE: EXECUTION OUTPUT -----\"\n",
    "        meta_data_file += exe_output_source\n",
    "        meta_data_file += \"\\n\\n\\n----- GENERATED: EXECUTION OUTPUT -----\"\n",
    "        meta_data_file += exe_output\n",
    "\n",
    "    # Write meta data for this experiment\n",
    "    filename = directory + \"/\" + \"metadata\" + \"_\" + file_name + \"__\" + str(dt_string) + \".txt\"\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(meta_data_file)\n",
    "    file.close()\n",
    "\n",
    "    print(\"Metadata file: \" + str(filename))\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b67740-220a-4ff6-a48b-501654cbdb46",
   "metadata": {},
   "source": [
    "## Code Generation Experiment - Setup + Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd36994f-8569-47c7-83c2-7cc7b12b1da4",
   "metadata": {},
   "source": [
    "_____\n",
    "***STEP 1/2: Input experimental configuration parameters in the following method.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62e4eab8-7be4-4423-9e74-c420ef8f51a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experimental_setup():\n",
    "    \"\"\"\n",
    "    [1] ENTER APPLICATION FOLDER NAME FROM HeCBench\n",
    "        >> Execution input parameters provided.\n",
    "\n",
    "    -- matrix-rotate [\"10000\", \"1\"]\n",
    "    -- jacobi [\"\"] (none required)\n",
    "    -- DNU -- mallocFree [\"10000\"]\n",
    "    -- layout [\"1\"]\n",
    "    -- atomicCost [\"1\"]\n",
    "    -- dense-embedding [\"10000\", \"8\", \"1\"]\n",
    "    -- pathfinder [\"10000\", \"1000\", \"1000\"]\n",
    "    -- bsearch [\"10000\", \"1\"]\n",
    "    -- entropy [\"10000\", \"1024\", \"1\"]\n",
    "    -- colorwheel [\"10000\", \"8\", \"1\"]\n",
    "    -- randomAccess [\"1\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    app_folder_name = \"randomAccess\"\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    [2] SELECT THE CODE TRANSLATION MAPPING DIRECTION EXPERIMENT CONFIGURATION\n",
    "    \n",
    "    CUDA -- OMP -- HIP -- SYCL -- FORTRAN\n",
    "    \"\"\"   \n",
    "    source_lang = \"CUDA\"\n",
    "    target_lang = \"OMP\"\n",
    "\n",
    "    # TOGGLE USE CODE KNOWLEDGE CONTEXT FOR TARGET\n",
    "    use_context = True  # True or False\n",
    "\n",
    "    # ASSIGN CODE COMPILE COMMAND\n",
    "    if target_lang == \"CUDA\":\n",
    "        code_compiler = \"nvcc\"\n",
    "        code_compiler_kwds = \"-std=c++14 -Xcompiler -Wall -arch=sm_80 -O3\"\n",
    "        # -arch=sm_70 for aiops01 GPU (Argonne)  # -arch=sm_80 for A100s\n",
    "        context_knowledge_selection = \"cuda\"\n",
    "    elif target_lang == \"OMP\":\n",
    "        # Linux_x86_64 or Linux_aarch64\n",
    "        code_compiler = \"/opt/nvidia/hpc_sdk/Linux_aarch64/2024/compilers/bin/nvc++\"  # \"g++\"\n",
    "        code_compiler_kwds = \"-Wall -O3 -Minfo -mp=gpu -gpu=cc80\"  # \"-fopenmp -O3 -c\"\n",
    "        context_knowledge_selection = \"omp\"\n",
    "\n",
    "    # TOGGLE EXECUTE PIPELINE\n",
    "    yes_execute = True  # True / False\n",
    "    if yes_execute:\n",
    "        # Enter execution input parameters, if needed\n",
    "        # A list of parameter string values, e.g., [\"10000\", \"1\"]\n",
    "        exe_input_parameters = [\"1\"]\n",
    "    else:\n",
    "        exe_input_parameters = [\"\"]\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    [3] SELECT THE LLM TO USE FOR THE CODE TRANSLATION EXPERIMENT\n",
    "    \"\"\"\n",
    "    # SELECT A MODEL\n",
    "    \n",
    "    # -- FROM LMSTUDIO\n",
    "    # model = \"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF/\"\n",
    "    # model = \"TheBloke/CodeLlama-13B-Instruct-GGUF\"\n",
    "    # model = \"lmstudio-community/starcoder2-15b-instruct-v0.1-GGUF\" - couldn;t generate anything\n",
    "    # model = \"lmstudio-community/stable-code-instruct-3b-GGUF\" - couldn't compile, but 'worked'\n",
    "    # model = \"microsoft/Phi-3-mini-4k-instruct-gguf\" - generated garbage, and slow\n",
    "    # model = \"lmstudio-community/codegemma-1.1-7b-it-GGUF\" - generated code, faster, but not compiled -- more tries\n",
    "    # model = \"lmstudio-community/Codestral-22B-v0.1-GGUF\"  # - nice large context size 32768 -- load it on aiop01 at 28 gpu layers\n",
    "\n",
    "    # -- FROM OLLAMA\n",
    "    # model = \"codestral:22b-v0.1-q8_0\"  # (max token = 32768)\n",
    "    model = \"wizardcoder:33b-v1.1-q8_0\"  # (max token = 16384)\n",
    "    # model = \"deepseek-coder-v2:16b-lite-instruct-fp16\"  # (max token = 128000)\n",
    "    # model = \"deepseek-coder-v2:16b-lite-instruct-q8_0\"\n",
    "  \n",
    "    # Set maximum context token count size for selected LLM:\n",
    "    num_ctx = 16384\n",
    "\n",
    "    # ASSIGN LLM API SOURCE:\n",
    "    if \"GGUF\" in model:\n",
    "        llm_source = \"lmstudio\"\n",
    "    else:\n",
    "        llm_source = \"ollama\"\n",
    "\n",
    "    \"\"\"\n",
    "    [4] SET ADDITIONAL PIPELINE PARAMETERS\n",
    "    \"\"\"\n",
    "    experiment = \"01a_withcontext\"  # Assign any label for experiment name.\n",
    "    display_response = True\n",
    "    test_with_error = False\n",
    "    generate_test_case = False\n",
    "\n",
    "    # Unload any previously loaded LLMs in the Ollama server \n",
    "    # (clear out the memory)\n",
    "    if llm_source == \"ollama\":\n",
    "        unload_ollama_model(model)\n",
    "\n",
    "    return (app_folder_name, source_lang, target_lang, model, num_ctx, llm_source, use_context,\n",
    "            context_knowledge_selection, code_compiler, code_compiler_kwds, yes_execute, exe_input_parameters,\n",
    "            experiment, display_response, test_with_error, generate_test_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2a4a00-c72a-4e5a-bfd9-d0f6cc45a2dd",
   "metadata": {},
   "source": [
    "_____\n",
    "***STEP 2/2: Run the following block to implement the pipeline with the configuration parameters.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a66833-3bc6-44e3-9138-7d574a821cfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load prompt dictionary (to capture recent manual changes to script, if any)\n",
    "from prompt_dictionary import PromptDictionary\n",
    "prompts = PromptDictionary()\n",
    "\n",
    "# Store the current home working directory (needed for code execution)\n",
    "original_home_directory = os.getcwd()\n",
    "\n",
    "\n",
    "# COLLECT THE EXPERIMENT CONFIGURATIONS\n",
    "(\n",
    "    app_folder_name, source_lang, target_lang, model, num_ctx,\n",
    "    llm_source, use_context, context_knowledge_selection, code_compiler,\n",
    "    code_compiler_kwds, yes_execute, exe_input_parameters, experiment,\n",
    "    display_response, test_with_error, generate_test_case\n",
    ") = experimental_setup()\n",
    "\n",
    "\n",
    "# IMPORT CONTEXT KNOWLEDGE FOR TARGET PROGRAMMING LANGUAGE\n",
    "input_context = prompts.get_codeknowledge(str(context_knowledge_selection))\n",
    "\n",
    "\n",
    "# DEFINE PROMPT TEMPLATES\n",
    "system_prompt_selection_index = source_lang + \"_to_\" + target_lang  # Select which prompt to use from the prompt dictionary\n",
    "this_llm_system_prompt = prompts.get_system_prompt(str(system_prompt_selection_index))\n",
    "\n",
    "codetranslate_prompt_selection_index = source_lang + \"_to_\" + target_lang  # Select which prompt to use from the prompt dictionary\n",
    "this_llm_translatecode_prompt = prompts.get_codetranslate_prompt(str(codetranslate_prompt_selection_index))\n",
    "\n",
    "\n",
    "# LOAD INPUT CODE TO TRANSLATE\n",
    "input_code_folder = \"HeCBench/\" + app_folder_name + \"/\"\n",
    "input_code_filename = app_folder_name + \"-\" + source_lang.lower() + \"_main\"\n",
    "if source_lang == \"OMP\" or source_lang == \"SYCL\":\n",
    "    input_code_extension = \"cpp\"\n",
    "elif source_lang == \"CUDA\" or source_lang == \"HIP\":\n",
    "    input_code_extension = \"cu\"\n",
    "source_file_directory = \"translated_code/input_codes/\" + input_code_folder\n",
    "source_file_path = source_file_directory + input_code_filename + \".\" + input_code_extension\n",
    "with open(source_file_path, 'r') as file:\n",
    "    input_code = file.read()\n",
    "file.close()\n",
    "\n",
    "\n",
    "# LOAD TARGET CODE TO COMPARE TRANSLATION\n",
    "target_code_filename = app_folder_name + \"-\" + target_lang.lower() + \"_main\"\n",
    "if target_lang == \"OMP\" or target_lang == \"SYCL\":\n",
    "    translated_code_extension = \"cpp\"\n",
    "elif target_lang == \"CUDA\" or target_lang == \"HIP\":\n",
    "    translated_code_extension = \"cu\"\n",
    "target_file_directory = \"translated_code/input_codes/\" + input_code_folder\n",
    "target_file_path = target_file_directory + target_code_filename + \".\" + translated_code_extension\n",
    "with open(target_file_path, 'r') as file:\n",
    "    target_code = file.read()\n",
    "file.close()\n",
    "\n",
    "\n",
    "# SPECIFY OUTPUT CODE SAVE\n",
    "directory = \"translated_code/\" + llm_source + \"_\" + model.replace('/', '--') + \"/promptset_\" + str(codetranslate_prompt_selection_index)\n",
    "file_name = input_code_filename + \"_translatecode_\" + target_lang + \"_exp\" + experiment\n",
    "\n",
    "\n",
    "# CREATE METADATA RECORD CONTENT\n",
    "meta_data_file = \"***** EXPERIMENT CONFIGURATION SUMMARY *****\\n\"\n",
    "meta_data_file += \"\\nApplication: \" + app_folder_name\n",
    "meta_data_file += \"\\n\\nTranslation: \" + system_prompt_selection_index\n",
    "meta_data_file += \"\\nTranslate from: \" + input_code_extension\n",
    "meta_data_file += \"\\nTranslate to: \" + translated_code_extension\n",
    "meta_data_file += \"\\n\\nLLM Source: \" + llm_source\n",
    "meta_data_file += \"\\nModel: \" + model\n",
    "meta_data_file += \"\\nSystem Prompt Index: \" + str(codetranslate_prompt_selection_index)\n",
    "meta_data_file += \"\\nCode file template: \" + directory + \"/\" + file_name\n",
    "meta_data_file += \"\\n\\nLLM System Prompt: \" + \"\\n\\n\" + this_llm_system_prompt\n",
    "meta_data_file += \"\\n\\nLLM Prompt: \" + \"\\n\\n\" + this_llm_translatecode_prompt\n",
    "\n",
    "print(\"\\n\" + meta_data_file + \"\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n\" + color.BOLD + \"***** BEGIN PIPELINE*****\" + color.END + \"\\n\\n\")\n",
    "\n",
    "# RUN PIPELINE\n",
    "complete = auto_code_llmgeneration_pipeline(\n",
    "    this_llm_system_prompt,\n",
    "    this_llm_translatecode_prompt,\n",
    "    llm_source, \n",
    "    model,\n",
    "    num_ctx,\n",
    "    # arrayormatrix_size_range,\n",
    "    # thread_count_range,\n",
    "    use_context,\n",
    "    input_context,\n",
    "    input_code,\n",
    "    target_code,\n",
    "    translated_code_extension,\n",
    "    target_lang,\n",
    "    code_compiler,\n",
    "    code_compiler_kwds,\n",
    "    directory,\n",
    "    file_name,\n",
    "    target_file_path,\n",
    "    display_response,\n",
    "    test_with_error,\n",
    "    generate_test_case,\n",
    "    yes_execute,\n",
    "    exe_input_parameters,\n",
    "    meta_data_file\n",
    ")\n",
    "\n",
    "# Reset to original working directory\n",
    "os.chdir(original_home_directory)\n",
    "\n",
    "if complete:\n",
    "    print(\"\\n\\n\" + color.GREEN + color.BOLD + \"***** PIPELINE COMPLETED *****\\n\\n\" + color.END)\n",
    "else:\n",
    "    print(\"\\n\\n\" + color.RED + color.BOLD + \"***** PIPELINE FAILED TO COMPLETE *****\\n\\n\" + color.END)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM4Codes Env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
