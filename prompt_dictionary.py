class PromptDictionary:
    def __init__(self):
        
        # Code Translation SYSTEM Prompts
        self.system_prompt_dict = {
            "general_system": "You are a professional coding AI assistant that specializes in translating parallelized code between coding frameworks.",
            
            "OMP_to_SYCL": "You are a professional coding AI assistant that specializes in translating parallelized C++ code using OpenMP directives to C++ code using SYCL. Provide only complete code without explanation. Surround your new generated code with the three characters ```.",
            
            "SYCL_to_OMP": "You are a professional coding AI assistant that specializes in translating parallelized C++ code using SYCL to C++ code using OpenMP directives. Include offload directives as needed to ensure the code can run on GPUs. Provide only complete code without explanation. Surround your new generated code with the three characters ```.",

            # "OMP_to_CUDA": "You are a professional coding AI assistant that specializes in translating parallelized C++ code using OpenMP directives to the CUDA framework. Provide only complete code without explanation. Avoid code methods that only include comments or references to source code. Surround your new generated code with the three characters ```.",

            "OMP_to_CUDA": "You are a professional coding AI assistant that specializes in translating parallelized C++ code using OpenMP directives to the CUDA framework. Always provide the complete and fully functional translated code without placeholders, comments, or references suggesting that parts of the original code should be included. Ensure every part of the translated code is explicitly written out. Surround your new generated code with the three characters ```.",
            
            # "CUDA_to_OMP": "You are a professional coding AI assistant that specializes in translating parallelized CUDA code to C++ code using OpenMP directives. Include offload directives as needed to ensure the code can run on GPUs. Use OpenMP static scheduling when needed. Provide only complete code without explanation. Surround your new generated code with the three characters ```.",

            "CUDA_to_OMP": "You are a professional coding AI assistant that specializes in translating parallelized CUDA code to C++ code using OpenMP directives. Always provide the complete and fully functional translated code without placeholders, comments, or references suggesting that parts of the original code should be included. Ensure every part of the translated code is explicitly written out. Surround your new generated code with the three characters ```.",
            
            "SYCL_to_CUDA": "You are a professional coding AI assistant that specializes in translating parallelized C++ code using SYCL to the CUDA framework. Provide only complete code without explanation. Surround your new generated code with the three characters ```.",

            "CUDA_to_SYCL": "You are a professional coding AI assistant that specializes in translating parallelized CUDA code to C++ code using SYCL. Provide only complete code without explanation. Surround your new generated code with the three characters ```.",
            
        }
        
        # Code Translation TASK Prompts
        self.codetranslate_prompt_dict = {

            # "OMP_to_CUDA": "Generate new code to refactor the following parallelized C++ program written with OpenMP to instead use the CUDA framework. Please avoid explaination of the code.",

            "OMP_to_CUDA": "Generate new code to refactor the following parallelized C++ program written with OpenMP to instead use the CUDA framework. Provide the complete translated CUDA code without any placeholders, comments, or references suggesting that parts of the original code should be included. Every part of the translated code should be explicitly written out. Avoid explanation of the code.",

            # "CUDA_to_OMP": "Generate new code to refactor the following parallelized CUDA program to instead use C++ code written with OpenMP directives. To enable GPU offloading, be sure to use the omp pragma directive 'target teams' for distributing for loop computations. Use static scheduling when needed. Please avoid dynamic scheduling. Please avoid explaination of the code.", 

            "CUDA_to_OMP": "Generate new code to refactor the following parallelized CUDA program to instead use C++ code written with OpenMP directives. To enable GPU offloading, use the `omp pragma` directive `target teams` for distributing `for` loop computations. Use static scheduling when needed and avoid dynamic scheduling. Provide the complete translated C++ code without any placeholders, comments, or references suggesting that parts of the original code should be included. Every part of the translated code should be explicitly written out. Avoid explanation of the code.",

            "OMP_to_SYCL": "Translate the following parallelized C++ program written with OpenMP to instead use the SYCL framework.",
            
            "SYCL_to_OMP": "Translate the following parallelized C++ program written with the SYCL framework to instead use OpenMP.",
            
            "SYCL_to_CUDA": "Translate the following parallelized C++ program written with the SYCL framework to instead use the CUDA framework.",

            "CUDA_to_SYCL": "Translate the following parallelized CUDA program to instead use C++ code written with the SYCL framework.",
        }

        # Context knowledge
        self.codeknowledge_dict = {

            "omp": "C/C++ OpenMP Application Program Interface (API) is a portable, scalable model that gives parallel programmers a simple and flexible interface for developing portable parallel applications. OpenMP supports multi-platform shared-memory parallel programming in C/C++ and Fortran on all architectures, including Unix platforms and Windows platforms. See www. openmp. org for specifications. 4. 0 Refers to functionality new in version 4. 0. [n. n. n] refers to sections in the OpenMP API specification version 4. 0, and [n. n. n] refers to version 3. 1. Directives An OpenMP executable directive applies to the succeeding structured block or an OpenMP construct. Each directive starts with #pragma omp. The remainder of the directive follows the conventions of the C and C++ standards for compiler directives. A structured-block is a single statement or a compound statement with a single entry at the top and a single exit at the bottom. parallel Forms a team of threads and starts parallel execution. #pragma omp parallel [clause[ [, ]clause]. . . ] structured-block clause: if (scalar-expression) num_threads (integer-expression) default (shared | none) private (list) firstprivate (list) shared (list) copyin (list) reduction (reduction-identifier: list) 4. 0 proc_bind (master | close | spread) loop [2. 7. 1] [2. 5. 1] Specifies that the iterations of associated loops will be executed in parallel by threads in the team in the context of their implicit tasks. #pragma omp for [clause[ [, ]clause]. . . ] for-loops clause: private (list) firstprivate (list) lastprivate (list) reduction (reduction-identifier: list) schedule (kind[, chunk_size]) collapse (n) ordered nowait kind:  static: Iterations are divided into chunks of size chunk_size and assigned to threads in the team in round-robin fashion in order of thread number.  dynamic: Each thread executes a chunk of iterations then requests another chunk until none remain.  guided: Each thread executes a chunk of iterations then requests another chunk until no chunks remain to be assigned.  auto: The decision regarding scheduling is delegated to the compiler and/or runtime system.  runtime: The schedule and chunk size are taken from the run-sched-var ICV. sections [2. 7. 2] [2. 5. 2] A noniterative worksharing construct that contains a set of structured blocks that are to be distributed among and executed by the threads in a team. #pragma omp sections [clause[ [, ] clause]. . . ] { [#pragma omp section] structured-block [#pragma omp section structured-block]. . . } clause: private (list) firstprivate (list) lastprivate (list) reduction (reduction-identifier: list) nowait single [2. 7. 3] [2. 5. 3] Specifies that the associated structured block is executed by only one of the threads in the team. #pragma omp single [clause[ [, ]clause]. . . ] structured-block clause: private (list) firstprivate (list) copyprivate (list) nowait 4. 0 simd [2. 8. 1] Applied to a loop to indicate that the loop can be transformed into a SIMD loop. #pragma omp simd [clause[ [, ]clause]. . . ] for-loops clause: safelen (length) linear (list[: linear-step]) aligned (list[: alignment]) private (list) lastprivate (list) reduction (reduction-identifier: list) collapse (n) 4. 0 declare simd [2. 8. 2] Enables the creation of one or more versions that can process multiple arguments using SIMD instructions from a single invocation from a SIMD loop. #pragma omp declare simd [clause[ [, ]clause]. . . ] [#pragma omp declare simd [clause[ [, ]clause]. . . ] ] [. . . ] function definition or declaration clause: simdlen (length) linear (argument-list[: constant-linear-step]) aligned (argument-list[: alignment]) uniform (argument-list) inbranch notinbranch 4. 0 loop simd [2. 8. 3] Specifies that a loop that can be executed concurrently using SIMD instructions, and that those iterations will also be executed in parallel by threads in the team. #pragma omp for simd [clause[ [, ]clause]. . . ] for-loops clause: Any accepted by the simd or for directives with identical meanings and restrictions. 4. 0 target [data] [2. 9. 1, 2. 9. 2] These constructs create a device data environment for the extent of the region. target also starts execution on the device. #pragma omp target data [clause[ [, ]clause]. . . ] structured-block #pragma omp target [clause[ [, ]clause]. . . ] structured-block clause: device (integer-expression) map ([map-type: ] list) if (scalar-expression) 4. 0 target update [2. 9. 3] Makes the corresponding list items in the device data environment consistent with their original list items, according to the specified motion clauses. #pragma omp target update clause[ [, ]clause], . . . ] clause is motion-clause or one of: device (integer-expression) if (scalar-expression) motion-clause: to (list) from (list) 4. 0 declare target [2. 9. 4] A declarative directive that specifies that variables and functions are mapped to a device. #pragma omp declare target declarations-definition-seq#pragma omp end declare target 4. 0 teams [2. 9. 5] Creates a league of thread teams where the master thread of each team executes the region. #pragma omp teams [clause[ [, ]clause], . . . ] structured-block clause: num_teams (integer-expression) thread_limit (integer-expression) default (shared | none) private (list) firstprivate (list) shared (list) reduction (reduction-identifier: list) 4. 0 distribute [simd] [2. 9. 6, 2. 9. 7] distribute specifies loops which are executed by the thread teams. distribute simd specifies loops which are executed concurrently using SIMD instructions. #pragma omp distribute [clause[ [, ]clause]. . . ] for-loops #pragma omp distribute simd [clause[ [, ]clause]. . . ] for-loops clause: private (list) firstprivate (list) collapse (n) dist_schedule (kind[, chunk_size]) 4. 0 distribute parallel for [simd] [2. 9. 8, 2. 9. 9] These constructs specify a loop that can be executed in parallel [using SIMD semantics in the simd case] by multiple threads that are members of multiple teams. #pragma omp distribute parallel for [clause[ [, ]clause]. . . ] for-loops #pragma omp distribute parallel for simd [clause[ [, ]clause]. . . ] for-loops clause: See clause for distribute Continued4OpenMP API 4. 0 C/C++ Page 2  2013 OpenMP ARB OMP1013C Directives (Continued) parallel loop [2. 10. 1] [2. 6. 1] Shortcut for specifying a parallel construct containing one or more associated loops and no other statements. #pragma omp parallel for [clause[ [, ]clause]. . . ] for-loop clause: Any accepted by the parallel or for directives, except the nowait clause, with identical meanings and restrictions. parallel sections [2. 10. 2] [2. 6. 2] Shortcut for specifying a parallel construct containing one sections construct and no other statements. #pragma omp parallel sections [clause[ [, ]clause]. . . ] { [#pragma omp section] structured-block [#pragma omp section structured-block]. . . } clause: Any of the clauses accepted by the parallel or sections directives, except the nowait clause, with identical meanings and restrictions. 4. 0 parallel loop simd [2. 10. 4] Shortcut for specifying a parallel construct containing one loop SIMD construct and no other statements. #pragma omp parallel for simd [clause[ [, ]clause]. . . ] for-loops clause: Any accepted by the parallel, for or simd directives, except the nowait clause, with identical meanings and restrictions. 4. 0 target teams [2. 10. 5] Shortcut for specifying a target construct containing a teams construct. #pragma omp target teams [clause[ [, ]clause]. . . ] structured-block clause: See clause for target or teams 4. 0 teams distribute [simd] [2. 10. 6, 2. 10. 7] Shortcuts for specifying a teams construct containing a distribute [simd] construct. #pragma omp teams distribute [clause[ [, ]clause]. . . ] for-loops #pragma omp teams distribute simd [clause[ [, ]clause]. . . ] for-loops clause: Any clause used for teams or distribute [simd] 4. 0 target teams distribute [simd] [2. 10. 8, 2. 10. 9] Shortcuts for specifying a target construct containing a teams distribute [simd] construct. #pragma omp target teams distribute [clause[ [, ]clause]. . . ] for-loops #pragma omp target teams distribute simd [clause[ [, ]clause]. . . ] for-loops clause: Any clause used for target or teams distribute [simd] 4. 0 teams distribute parallel for [simd] [2. 10. 10, 12] Shortcuts for specifying a teams construct containing a distribute parallel for [simd] construct. #pragma omp teams distribute parallel for [clause[ [, ]clause]. . . ] for-loops #pragma omp teams distribute parallel for simd [clause[ [, ]clause]. . . ] for-loops clause: Any clause used for teams or distribute parallel for [simd] 4. 0 target teams distribute parallel for [simd] [2. 10. 11, 13] Shortcut for specifying a target construct containing a teams distribute parallel for [simd] construct. #pragma omp target teams distribute parallel for \\ [clause[ [, ]clause]. . . ] for-loops #pragma omp target teams distribute parallel for simd \\ [clause[ [, ]clause]. . . ] for-loops clause: Any clause used for target or teams distribute parallel for [simd]task [2. 11. 1] [2. 7. 1] Defines an explicit task. The data environment of the task is created according to data-sharing attribute clauses on task construct and any defaults that apply. #pragma omp task [clause[ [, ]clause]. . . ] structured-block clause: if (scalar-expression) final (scalar-expression) untied default (shared | none) mergeableprivate (list) firstprivate (list) shared (list) 4. 0 depend (dependence-type: list) The list items that appear in the depend clause may include array sections. dependence-type: The generated task will be a dependent task of all previously generated sibling tasks that reference at least one of the list items. . .  in: . . . in an out or inout clause.  out and inout: . . . in an in, out, or inout clause. taskyield [2. 11. 2] [2. 7. 2] Specifies that the current task can be suspended in favor of execution of a different task. #pragma omp taskyield master [2. 12. 1] [2. 8. 1] Specifies a structured block that is executed by the master thread of the team. #pragma omp master structured-block critical [2. 12. 2] [2. 8. 2] Restricts execution of the associated structured block to a single thread at a time. #pragma omp critical [ (name) ] structured-block barrier [2. 12. 3] [2. 8. 3] Specifies an explicit barrier at the point at which the construct appears. #pragma omp barrier taskwait [2. 12. 4] [2. 8. 4], 4. 0 taskgroup [2. 12. 5] These constructs each specify a wait on the completion of child tasks of the current task. taskgroup also waits for descendant tasks. #pragma omp taskwait #pragma omp taskgroup structured-block atomic [2. 12. 6] [2. 8. 5] Ensures that a specific storage location is accessed atomically. [seq_cst] is 4. 0. #pragma omp atomic [ read | write | update | capture] [seq_cst] expression-stmt #pragma omp atomic capture [ seq_cst] structured-block where expression-stmt may be one of: if clause is. . . expression-stmt: read v = x; write x = expr; update or is not present x++; x--; ++x; --x; x binop= expr; x = x binop expr; x = expr binop x; capture v =x++; v=x--; v=++x; v= --x; v=x binop= expr; v=x = x binop expr; v=x = expr binop x; (Continued >) atomic (continued) and where structured-block may be one of the following forms: {v = x; x binop= expr; } {x binop= expr; v = x; } {v = x; x = x binop expr; } {v = x; x = expr binop x; } {x = x binop expr; v = x; } {x = expr binop x; v = x; } {v = x; x = expr; } {v = x; x++; } {v = x; ++x; } {++x; v = x; } {x++; v = x; } {v = x; x--; } {v = x; --x; } {--x; v = x; } {x--; v = x; } flush [2. 12. 7] [2. 8. 6] Executes the OpenMP flush operation, which makes a threads temporary view of memory consistent with memory, and enforces an order on the memory operations of the variables. #pragma omp flush [ (list) ] ordered [2. 12. 8] [2. 8. 7] Specifies a structured block in a loop region that will be executed in the order of the loop iterations. #pragma omp ordered structured-block 4. 0 cancel [2. 13. 1] Requests cancellation of the innermost enclosing region of the type specified. The cancel directive may not be used in place of the statement following an if, while, do, switch, or label. #pragma omp cancel construct-type-clause[ [, ] if-clause] construct-type-clause: parallel sectionsfortaskgroup if-clause: if (scalar-expression) 4. 0 cancellation point [2. 13. 2] Introduces a user-defined cancellation point at which tasks check if cancellation of the innermost enclosing region of the type specified has been requested. #pragma omp cancellation point construct-type-clause construct-type-clause: parallel sectionsfortaskgroup threadprivate [2. 14. 2] [2. 9. 2] Specifies that variables are replicated, with each thread having its own copy. Each copy of a threadprivate variable is initialized once prior to the first reference to that copy. #pragma omp threadprivate (list) list: A comma-separated list of file-scope, namespace-scope, or static block-scope variables that do not have incomplete types. 4. 0 declare reduction [2. 15] Declares a reduction-identifier that can be used in a reduction clause. #pragma omp declare reduction (reduction-identifier: typename-list: combiner) [initializer-clause] reduction-identifier: A base language identifier or one of the following operators: +, -, *, &, |, ^, && and || In C++, this may also be an operator-function-id typename-list: A list of type names combiner: An expressioninitializer-clause: initializer (omp_priv = initializer | function-name (argument-list)) 2013 OpenMP ARB OMP1013COpenMP API 4. 0 C/C++ Page 3 Runtime Library Routines Return types are shown in green. Execution environment routines affect and monitor threads, processors, and the parallel environment. The library routines are external functions with C linkage. Execution Environment Routines omp_set_num_threads [3. 2. 1] [3. 2. 1] Affects the number of threads used for subsequent parallel regions not specifying a num_threads clause, by setting the value of the first element of the nthreads-var ICV of the current task to num_threads. void omp_set_num_threads (int num_threads); omp_get_num_threads [3. 2. 2] [3. 2. 2] Returns the number of threads in the current team. The binding region for an omp_get_num_threads region is the innermost enclosing parallel region. int omp_get_num_threads (void); omp_get_max_threads [3. 2. 3] [3. 2. 3] Returns an upper bound on the number of threads that could be used to form a new team if a parallel construct without a num_threads clause were encountered after execution returns from this routine. int omp_get_max_threads (void); omp_get_thread_num [3. 2. 4] [3. 2. 4] Returns the thread number of the calling thread within the current team. int omp_get_thread_num (void); omp_get_num_procs [3. 2. 5] [3. 2. 5] Returns the number of processors that are available to the device at the time the routine is called. int omp_get_num_procs (void); omp_in_parallel [3. 2. 6] [3. 2. 6] Returns true if the active-levels-var ICV is greater than zero; otherwise it returns false. int omp_in_parallel (void); omp_set_dynamic [3. 2. 7] [3. 2. 7] Returns the value of the dyn-var ICV, which indicates if dynamic adjustment of the number of threads is enabled or disabled. void omp_set_dynamic (int dynamic_threads); omp_get_dynamic [3. 2. 8] [3. 2. 8] This routine returns the value of the dyn-var ICV, which is true if dynamic adjustment of the number of threads is enabled for the current task. int omp_get_dynamic (void); 4. 0 omp_get_cancellation [3. 2. 9] Returns the value of the cancel-var ICV, which controls the behavior of cancel construct and cancellation points. int omp_get_cancellation (void); omp_set_nested [3. 2. 10] [3. 2. 9] Enables or disables nested parallelism, by setting the nest-var ICV. void omp_set_nested (int nested); omp_get_nested [3. 2. 11] [3. 2. 10] Returns the value of the nest-var ICV, which indicates if nested parallelism is enabled or disabled. int omp_get_nested (void); omp_set_schedule [3. 2. 12] [3. 2. 11] Affects the schedule that is applied when runtime is used as schedule kind. void omp_set_schedule (omp_sched_t kind, int modifier); kind: one of the folowing, or an implementation-defined schedule: omp_sched_static = 1 omp_sched_dynamic = 2 omp_sched_guided = 3 omp_sched_auto = 4omp_get_schedule [3. 2. 13] [3. 2. 12] Returns the value of run-sched-var ICV, which is the schedule applied when runtime schedule is used. void omp_get_schedule (omp_sched_t *kind, int *modifier); See kind above. omp_get_thread_limit [3. 2. 14] [3. 2. 13] Returns the value of the thread-limit-var ICV, which is the maximum number of OpenMP threads available. int omp_get_thread_limit (void); omp_set_max_active_levels [3. 2. 15] [3. 2. 14] Limits the number of nested active parallel regions, by setting max-active-levels-var ICV. void omp_set_max_active_levels (int max_levels); omp_get_max_active_levels [3. 2. 16] [3. 2. 15] Returns the value of max-active-levels-var ICV, which determines the maximum number of nested active parallel regions. int omp_get_max_active_levels (void); omp_get_level [3. 2. 17] [3. 2. 16] For the enclosing device region, returns the levels-vars ICV, which is the number of nested parallel regions that enclose the task containing the call. int omp_get_level (void); omp_get_ancestor_thread_num [3. 2. 18] [3. 2. 17] Returns, for a given nested level of the current thread, the thread number of the ancestor of the current thread. int omp_get_ancestor_thread_num (int level); omp_get_team_size [3. 2. 19] [3. 2. 18] Returns, for a given nested level of the current thread, the size of the thread team to which the ancestor or the current thread belongs. int omp_get_team_size (int level); omp_get_active_level [3. 2. 20] [3. 2. 19] Returns the value of the active-level-vars ICV, which determines the number of active, nested parallel regions enclosing the task that contains the call. int omp_get_active_level (void); omp_in_final [3. 2. 21] [3. 2. 20] Returns true if the routine is executed in a final task region; otherwise, it returns false. int omp_in_final (void); 4. 0 omp_get_proc_bind [3. 2. 22] Returns the thread affinity policy to be used for the subsequent nested parallel regions that do not specify a proc_bind clause. omp_proc_bind_t omp_get_proc_bind (void); Returns one of: omp_proc_bind_false = 0 omp_proc_bind_true = 1 omp_proc_bind_master = 2 omp_proc_bind_close = 3 omp_proc_bind_spread = 4 4. 0 omp_set_default_device [3. 2. 23] Controls the default target device byassigning the value of the default-device-var ICV. void omp_set_default_device (int device_num); 4. 0 omp_get_default_device [3. 2. 24] Returns the default target device. int omp_get_default_device (void); 4. 0 omp_get_num_devices [3. 2. 25] Returns the number of target devices. int omp_get_num_devices (void); 4. 0 omp_get_num_teams [3. 2. 26] Returns the number of teams in the current teams region, or 1 if called from outside of a teams region. int omp_get_num_teams (void); 4. 0 omp_get_team_num [3. 2. 27] Returns the team number of calling thread. The team number is an integer between 0 and one less than the value returned by omp_get_num_teams, inclusive. int omp_get_team_num (void); 4. 0 omp_is_initial_device [3. 2. 28] Returns true if the current task is executing on the host device; otherwise, it returns false. int omp_is_initial_device (void); Lock Routines General-purpose lock routines. Two types of locks are supported: simple locks and nestable locks. A nestable lock can be set multiple times by the same task before being unset; a simple lock cannot be set if it is already owned by the task trying to set it. Initialize lock [3. 3. 1] [3. 3. 1] Initialize an OpenMP lock. void omp_init_lock (omp_lock_t *lock); void omp_init_nest_lock (omp_nest_lock_t *lock); Destroy lock [3. 3. 2] [3. 3. 2] Ensure that the OpenMP lock is uninitialized. void omp_destroy_lock (omp_lock_t *lock); void omp_destroy_nest_lock (omp_nest_lock_t *lock); Set lock [3. 3. 3] [3. 3. 3] Sets an OpenMP lock. The calling task region is suspended until the lock is set. void omp_set_lock (omp_lock_t *lock); void omp_set_nest_lock (omp_nest_lock_t *lock); Unset lock [3. 3. 4] [3. 3. 4] Unsets an OpenMP lock. void omp_unset_lock (omp_lock_t *lock); void omp_unset_nest_lock (omp_nest_lock_t *lock); Test lock [3. 3. 5] [3. 3. 5] Attempt to set an OpenMP lock but do not suspend execution of the task executing the routine. int omp_test_lock (omp_lock_t *lock); int omp_test_nest_lock (omp_nest_lock_t *lock); Timing Routines Timing routines support a portable wall clock timer. These record elapsed time per-thread and are not guaranteed to be globally consistent across all the threads participating in an application. omp_get_wtime [3. 4. 1] [3. 4. 1] Returns elapsed wall clock time in seconds. double omp_get_wtime (void); omp_get_wtick [3. 4. 2] [3. 4. 2] Returns the precision of the timer (seconds between ticks) used by omp_get_wtime. double omp_get_wtick (void); OpenMP API 4. 0 C/C++ Page 4  2013 OpenMP ARB OMP1013C Clauses The set of clauses that is valid on a particular directive is described with the directive. Most clauses accept a comma-separated list of list items. All list items appearing in a clause must be visible, according to the scoping rules of the base language. Not all of the clauses listed in this section are valid on all directives. The set of clauses that is valid on a particular directive is described with the directive. Data Sharing Attribute Clauses [2. 14. 3] [2. 9. 3] Data-sharing attribute clauses apply only to variables whose names are visible in the construct on which the clause appears. default (shared | none) Explicitly determines the default data-sharing attributes of variables that are referenced in a parallel, task, or teams construct, causing all variables referenced in the construct that have implicitly determined data-sharing attributes to be shared. shared (list) Declares one or more list items to be shared by tasks generated by a parallel, task, or teams construct. The programmer must ensure that storage shared by an explicit task region does not reach the end of its lifetime before the explicit task region completes its execution. private (list) Declares one or more list items to be private to a task or a SIMD lane. Each task that references a list item that appears in a private clause in any statement in the construct receives a new list item. firstprivate (list) Declares list items to be private to a task, and initializes each of them with the value that the corresponding original item has when the construct is encountered. lastprivate (list) Declares one or more list items to be private to an implicit task or to a SIMD lane, and causes the corresponding original list item to be updated after the end of the region. 4. 0 linear (list[: linear-step]) Declares one or more list items to be private to a SIMD lane and to have a linear relationship with respect to the iteration space of a loop. reduction (reduction-identifier: list) Specifies a reduction-identifier and one or more list items. The reduction-identifier must match a previously declared reduction-identifier of the same name and type for each of the list items. Operators for reduction (initialization values) + (0) | (0) * (1) ^ (0) - (0) && (1) & (~0) || (0) max (Least representable number in reduction list item type) min (Largest representable number in reduction list item type) Data Copying Clauses [2. 14. 4] [2. 9. 4] copyin (list) Copies the value of the master threads threadprivate variable to the threadprivate variable of each other member of the team executing the parallel region. copyprivate (list) Broadcasts a value from the data environment of one implicit task to the data environments of the other implicit tasks belonging to the parallel region. 4. 0 Map Clause [2. 14. 5] map ([map-type: ]ist) Map a variable from the tasks data environment to the device data environment associated with the construct. map-type: alloc: On entry to the region each new corresponding list item has an undefined initial value. to: On entry to the region each new corresponding list item is initialized with the original list items value. from: On exit from the region the corresponding list items value is assigned to each original list item. (Continued >) tofrom: (Default) On entry to the region each new corresponding list item is initialized with the original list items value, and on exit from the region the corresponding list items value is assigned to each original list item. 4. 0 SIMD Clauses [2. 8. 1] safelen (length) If used then no two iterations executed concurrently with SIMD instructions can have a greater distance in the logical iteration space than its value. collapse (n) A constant positive integer expression that specifies how many loops are associated with the loop construct. simdlen (length) A constant positive integer expression that specifies the number of concurrent arguments of the function. aligned (argument-list[: alignment]) Declares one or more list items to be aligned to the specified number of bytes. alignment, if present, must be a constant positive integer expression. If no optional parameter is specified, implementation-defined default alignments for SIMD instructions on the target platforms are assumed. uniform (argument-list) Declares one or more arguments to have an invariant value for all concurrent invocations of the function in the execution of a single SIMD loop. inbranch Specifies that the function will always be called from inside a conditional statement of a SIMD loop. notinbranch Specifies that the function will never be called from inside a conditional statement of a SIMD loop. Environment Variables [4] Environment variable names are upper case, and the values assigned to them are case insensitive and may have leading and trailing white space. 4. 0 [4. 11] OMP_CANCELLATION policy Sets the cancel-var ICV. policy may be true or false. If true, the effects of the cancel construct and of cancellation points are enabled and cancellation is activated 4. 0 [4. 13] OMP_DEFAULT_DEVICE device Sets the default-device-var ICV that controls the default device number to use in device constructs. 4. 0 [4. 12] OMP_DISPLAY_ENV var If var is TRUE, instructs the runtime to display the OpenMP version number and the value of the ICVs associated with the environment variables as name =value pairs. If var is VERBOSE, the runtime may also display vendor-specific variables. If var is FALSE, no information is displayed. [4. 3] [4. 3] OMP_DYNAMIC dynamic Sets the dyn-var ICV. If true, the implementation may dynamically adjust the number of threads to use for executing parallel regions. [4. 9] [4. 8] OMP_MAX_ACTIVE_LEVELS levels Sets the max-active-levels-var ICV that controls the maximum number of nested active parallel regions. [4. 6] [4. 5] OMP_NESTED nested Sets the nest-var ICV to enable or to disable nested parallelism. Valid values for nested are true or false. [4. 2] [4. 2] OMP_NUM_THREADS list Sets the nthreads-var ICV for the number of threads to use for parallel regions. 4. 0 [4. 5] OMP_PLACES places Sets the place-partition-var ICV that defines the OpenMP places available to the execution environment. places is an abstract name (threads, cores, sockets, or imple-mentation-defined), or a list of non-negative numbers. [4. 4] [4. 4] OMP_PROC_BIND policy Sets the value of the global bind-var ICV, which sets the thread affinity policy to be used for parallel regions at the corresponding nested level. policy can be the values true, false, or a comma-separated list of master, close, or spread in quotes. [4. 1] [4. 1] OMP_SCHEDULE type[, chunk] Sets the run-sched-var ICV for the runtime schedule type and chunk size. Valid OpenMP schedule types are static, dynamic, guided, or auto. [4. 7] [4. 6] OMP_STACKSIZE size[B | K | M | G] Sets the stacksize-var ICV that specifies the size of the stack for threads created by the OpenMP implementation. size is a positive integer that specifies stack size. If unit is not specified, size is measured in kilobytes (K) . [4. 10] [4. 9] OMP_THREAD_LIMIT limit Sets the thread-limit-var ICV that controls the number of threads participating in the OpenMP program. [4. 8] [4. 7] OMP_WAIT_POLICY policy Sets the wait-policy-var ICV that provides a hint to an OpenMP implementation about the desired behavior of waiting threads. Valid values for policy are ACTIVE (waiting threads consume processor cycles while waiting) and PASSIVE.",

            "cuda": "Chapter 5. Programming Model This chapter introduces the main concepts behind the CUDA programming model by outlining how they are exposed in C++. An extensive description of CUDA C++ is given in Programming Interface. Full code for the vector addition example used in this chapter and the next can be found in the vectorAdd CUDA sample. 5. 1. Kernels CUDA C++ extends C++ by allowing the programmer to define C++ functions, called kernels, that, when called, are executed N times in parallel by N different CUDA threads, as opposed to only once like regular C++ functions. A kernel is defined using the __global__ declaration specifier and the number of CUDA threads that execute that kernel for a given kernel call is specified using a new <<<. . . >>> execution configuration syntax (see C++ Language Extensions) . Each thread that executes the kernel is given a unique thread IDthat is accessible within the kernel through built-in variables. As an illustration, the following sample code, using the built-in variable threadIdx, adds two vectors AandBof size Nand stores the result into vector C:  Kernel definition __global__ void VecAdd (float *A, float *B, float *C) { int i=threadIdx. x; C[i] =A[i] +B[i]; } int main () {. . .  Kernel invocation with N threads VecAdd <<<1, N>>> (A, B, C); . . . } Here, each of the Nthreads that execute VecAdd () performs one pair-wise addition. 11CUDA C++ Programming Guide, Release 12. 5 5. 2. Thread Hierarchy For convenience, threadIdx is a 3-component vector, so that threads can be identified using a onedimensional, two-dimensional, or three-dimensional thread index, forming a one-dimensional, twodimensional, or three-dimensional block of threads, called a thread block. This provides a natural way to invoke computation across the elements in a domain such as a vector, matrix, or volume. The index of a thread and its thread ID relate to each other in a straightforward way: For a onedimensional block, they are the same; for a two-dimensional block of size (Dx, Dy), the thread ID of a thread of index (x, y) is (x + y Dx); for a three-dimensional block of size (Dx, Dy, Dz), the thread ID of a thread of index (x, y, z) is (x + y Dx + z Dx Dy) . As an example, the following code adds two matrices AandBof size NxNand stores the result into matrix C:  Kernel definition __global__ void MatAdd (float A[N][N], float B[N][N], float C[N][N]) { int i=threadIdx. x; int j=threadIdx. y; C[i][j] =A[i][j] +B[i][j]; } int main () {. . .  Kernel invocation with one block of N * N * 1 threads int numBlocks =1; dim3 threadsPerBlock (N, N); MatAdd <<<numBlocks, threadsPerBlock >>> (A, B, C); . . . } There is a limit to the number of threads per block, since all threads of a block are expected to reside on the same streaming multiprocessor core and must share the limited memory resources of that core. On current GPUs, a thread block may contain up to 1024 threads. However, a kernel can be executed by multiple equally-shaped thread blocks, so that the total number of threads is equal to the number of threads per block times the number of blocks. Blocks are organized into a one-dimensional, two-dimensional, or three-dimensional gridof thread blocks as illustrated by Figure 4. The number of thread blocks in a grid is usually dictated by the size of the data being processed, which typically exceeds the number of processors in the system. The number of threads per block and the number of blocks per grid specified in the <<<. . . >>> syntax can be of type intordim3. Two-dimensional blocks or grids can be specified as in the example above. Each block within the grid can be identified by a one-dimensional, two-dimensional, or threedimensional unique index accessible within the kernel through the built-in blockIdx variable. The dimension of the thread block is accessible within the kernel through the built-in blockDim variable. Extending the previous MatAdd () example to handle multiple blocks, the code becomes as follows.  Kernel definition __global__ void MatAdd (float A[N][N], float B[N][N], float C[N][N]) (continues on next page) 12 Chapter 5. Programming ModelCUDA C++ Programming Guide, Release 12. 5 Figure 4: Grid of Thread Blocks (continued from previous page) { int i=blockIdx. x *blockDim. x +threadIdx. x; int j=blockIdx. y *blockDim. y +threadIdx. y; if (i<N&&j<N) C[i][j] =A[i][j] +B[i][j]; } int main () {. . .  Kernel invocation dim3 threadsPerBlock (16, 16); dim3 numBlocks (NthreadsPerBlock. x, NthreadsPerBlock. y); MatAdd <<<numBlocks, threadsPerBlock >>> (A, B, C); . . . } A thread block size of 16x16 (256 threads), although arbitrary in this case, is a common choice. The grid is created with enough blocks to have one thread per matrix element as before. For simplicity, this example assumes that the number of threads per grid in each dimension is evenly divisible by the number of threads per block in that dimension, although that need not be the case. Thread blocks are required to execute independently: It must be possible to execute them in any order, in parallel or in series. This independence requirement allows thread blocks to be scheduled in any order across any number of cores as illustrated by Figure 3, enabling programmers to write code that scales with the number of cores. Threads within a block can cooperate by sharing data through some shared memory and by synchronizing their execution to coordinate memory accesses. More precisely, one can specify synchronization points in the kernel by calling the __syncthreads () intrinsic function; __syncthreads () acts as a barrier at which all threads in the block must wait before any is allowed to proceed. Shared Memory gives an example of using shared memory. In addition to __syncthreads (), the Cooperative Groups APIprovides a rich set of thread-synchronization primitives. For efficient cooperation, the shared memory is expected to be a low-latency memory near each processor core (much like an L1 cache) and __syncthreads () is expected to be lightweight. 5. 2. Thread Hierarchy 13CUDA C++ Programming Guide, Release 12. 5 5. 2. 1. Thread Block Clusters With the introduction of NVIDIA Compute Capability 9. 0, the CUDA programming model introduces an optional level of hierarchy called Thread Block Clusters that are made up of thread blocks. Similar to how threads in a thread block are guaranteed to be co-scheduled on a streaming multiprocessor, thread blocks in a cluster are also guaranteed to be co-scheduled on a GPU Processing Cluster (GPC) in the GPU. Similar to thread blocks, clusters are also organized into a one-dimension, two-dimension, or threedimension as illustrated by Figure 5. The number of thread blocks in a cluster can be user-defined, and a maximum of 8 thread blocks in a cluster is supported as a portable cluster size in CUDA. Note that on GPU hardware or MIG configurations which are too small to support 8 multiprocessors the maximum cluster size will be reduced accordingly. Identification of these smaller configurations, as well as of larger configurations supporting a thread block cluster size beyond 8, is architecture-specific and can be queried using the cudaOccupancyMaxPotentialClusterSize API. Figure 5: Grid of Thread Block Clusters Note: In a kernel launched using cluster support, the gridDim variable still denotes the size in terms of number of thread blocks, for compatibility purposes. The rank of a block in a cluster can be found using the Cluster Group API. A thread block cluster can be enabled in a kernel either using a compiler time kernel attribute using __cluster_dims__ (X, Y, Z) or using the CUDA kernel launch API cudaLaunchKernelEx. The example below shows how to launch a cluster using compiler time kernel attribute. The cluster size using kernel attribute is fixed at compile time and then the kernel can be launched using the classical <<<, >>>. If a kernel uses compile-time cluster size, the cluster size cannot be modified when launching the kernel.  Kernel definition  Compile time cluster size 2 in X-dimension and 1 in Y and Z dimension __global__ void __cluster_dims__ (2, 1, 1) cluster_kernel (float *input, float *output) { } int main () { (continues on next page) 14 Chapter 5. Programming ModelCUDA C++ Programming Guide, Release 12. 5 (continued from previous page) float *input, *output;  Kernel invocation with compile time cluster size dim3 threadsPerBlock (16, 16); dim3 numBlocks (NthreadsPerBlock. x, NthreadsPerBlock. y);  The grid dimension is not affected by cluster launch, and is still enumerated  using number of blocks.  The grid dimension must be a multiple of cluster size. cluster_kernel <<<numBlocks, threadsPerBlock >>> (input, output); } A thread block cluster size can also be set at runtime and the kernel can be launched using the CUDA kernel launch API cudaLaunchKernelEx. The code example below shows how to launch a cluster kernel using the extensible API.  Kernel definition  No compile time attribute attached to the kernel __global__ void cluster_kernel (float *input, float *output) { } int main () { float *input, *output; dim3 threadsPerBlock (16, 16); dim3 numBlocks (NthreadsPerBlock. x, NthreadsPerBlock. y);  Kernel invocation with runtime cluster size { cudaLaunchConfig_t config ={0};  The grid dimension is not affected by cluster launch, and is still enumerated  using number of blocks.  The grid dimension should be a multiple of cluster size. config. gridDim =numBlocks; config. blockDim =threadsPerBlock; cudaLaunchAttribute attribute[ 1]; attribute[ 0]. id =cudaLaunchAttributeClusterDimension; attribute[ 0]. val. clusterDim. x =2; Cluster size in X-dimension attribute[ 0]. val. clusterDim. y =1; attribute[ 0]. val. clusterDim. z =1; config. attrs =attribute; config. numAttrs =1; cudaLaunchKernelEx (&config, cluster_kernel, input, output); } } In GPUs with compute capability 9. 0, all the thread blocks in the cluster are guaranteed to be coscheduled on a single GPU Processing Cluster (GPC) and allow thread blocks in the cluster to perform hardware-supported synchronization using the Cluster Group APIcluster. sync () . Cluster group also provides member functions to query cluster group size in terms of number of threads or number of blocks using num_threads () andnum_blocks () API respectively. The rank of a thread or block in the cluster group can be queried using dim_threads () anddim_blocks () API respectively. Thread blocks that belong to a cluster have access to the Distributed Shared Memory. Thread blocks 5. 2. Thread Hierarchy 15CUDA C++ Programming Guide, Release 12. 5 in a cluster have the ability to read, write, and perform atomics to any address in the distributed shared memory. Distributed Shared Memory gives an example of performing histograms in distributed shared memory. 5. 3. Memory Hierarchy CUDA threads may access data from multiple memory spaces during their execution as illustrated by Figure 6. Each thread has private local memory. Each thread block has shared memory visible to all threads of the block and with the same lifetime as the block. Thread blocks in a thread block cluster can perform read, write, and atomics operations on each others shared memory. All threads have access to the same global memory. There are also two additional read-only memory spaces accessible by all threads: the constant and texture memory spaces. The global, constant, and texture memory spaces are optimized for different memory usages (see Device Memory Accesses) . Texture memory also offers different addressing modes, as well as data filtering, for some specific data formats (see Texture and Surface Memory) . The global, constant, and texture memory spaces are persistent across kernel launches by the same application. Figure 6: Memory Hierarchy 16 Chapter 5. Programming ModelCUDA C++ Programming Guide, Release 12. 5 5. 4. Heterogeneous Programming As illustrated by Figure 7, the CUDA programming model assumes that the CUDA threads execute on a physically separate device that operates as a coprocessor to the host running the C++ program. This is the case, for example, when the kernels execute on a GPU and the rest of the C++ program executes on a CPU. The CUDA programming model also assumes that both the host and the device maintain their own separate memory spaces in DRAM, referred to as host memory anddevice memory, respectively. Therefore, a program manages the global, constant, and texture memory spaces visible to kernels through calls to the CUDA runtime (described in Programming Interface) . This includes device memory allocation and deallocation as well as data transfer between host and device memory. Unified Memory provides managed memory to bridge the host and device memory spaces. Managed memory is accessible from all CPUs and GPUs in the system as a single, coherent memory image with a common address space. This capability enables oversubscription of device memory and can greatly simplify the task of porting applications by eliminating the need to explicitly mirror data on host and device. See Unified Memory Programming for an introduction to Unified Memory. 5. 5. Asynchronous SIMT Programming Model In the CUDA programming model a thread is the lowest level of abstraction for doing a computation or a memory operation. Starting with devices based on the NVIDIA Ampere GPU architecture, the CUDA programming model provides acceleration to memory operations via the asynchronous programming model. The asynchronous programming model defines the behavior of asynchronous operations with respect to CUDA threads. The asynchronous programming model defines the behavior of Asynchronous Barrier for synchronization between CUDA threads. The model also explains and defines how cuda: : memcpy_async can be used to move data asynchronously from global memory while computing in the GPU. 5. 5. 1. Asynchronous Operations An asynchronous operation is defined as an operation that is initiated by a CUDA thread and is executed asynchronously as-if by another thread. In a well formed program one or more CUDA threads synchronize with the asynchronous operation. The CUDA thread that initiated the asynchronous operation is not required to be among the synchronizing threads. Such an asynchronous thread (an as-if thread) is always associated with the CUDA thread that initiated the asynchronous operation. An asynchronous operation uses a synchronization object to synchronize the completion of the operation. Such a synchronization object can be explicitly managed by a user (e. g. , cuda: : memcpy_async) or implicitly managed within a library (e. g. , cooperative_groups: : memcpy_async) . A synchronization object could be a cuda: : barrier or acuda: : pipeline. These objects are explained in detail in Asynchronous Barrier andAsynchronous Data Copies using cuda: : pipeline. These synchronization objects can be used at different thread scopes. A scope defines the set of threads that may use the synchronization object to synchronize with the asynchronous operation. The following table defines the thread scopes available in CUDA C++ and the threads that can be synchronized with each. 5. 4. Heterogeneous Programming 17CUDA C++ Programming Guide, Release 12. 5 Figure 7: Heterogeneous Programming Note: Serial code executes on the host while parallel code executes on the device. 18 Chapter 5. Programming ModelCUDA C++ Programming Guide, Release 12. 5 Thread Scope Description cuda: : thread_scope: : thread_scope_thread Only the CUDA thread which initiated asynchronous operations synchronizes. cuda: : thread_scope: : thread_scope_block All or any CUDA threads within the same thread block as the initiating thread synchronizes. cuda: : thread_scope: : thread_scope_device All or any CUDA threads in the same GPU device as the initiating thread synchronizes. cuda: : thread_scope: : thread_scope_system All or any CUDA or CPU threads in the same system as the initiating thread synchronizes. These thread scopes are implemented as extensions to standard C++ in the CUDA Standard C++ library. 5. 6. Compute Capability Thecompute capability of a device is represented by a version number, also sometimes called its SM version. This version number identifies the features supported by the GPU hardware and is used by applications at runtime to determine which hardware features and/or instructions are available on the present GPU. The compute capability comprises a major revision number Xand a minor revision number Yand is denoted by X. Y. Devices with the same major revision number are of the same core architecture. The major revision number is 9 for devices based on the NVIDIA Hopper GPU architecture, 8 for devices based on the NVIDIA Ampere GPU architecture, 7 for devices based on the Volta architecture, 6 for devices based on thePascal architecture, 5 for devices based on the Maxwell architecture, and 3 for devices based on theKepler architecture. The minor revision number corresponds to an incremental improvement to the core architecture, possibly including new features. Turing is the architecture for devices of compute capability 7. 5, and is an incremental update based on the Volta architecture. CUDA-Enabled GPUs lists of all CUDA-enabled devices along with their compute capability. Compute Capabilities gives the technical specifications of each compute capability. Note: The compute capability version of a particular GPU should not be confused with the CUDA version (for example, CUDA 7. 5, CUDA 8, CUDA 9), which is the version of the CUDA software platform. The CUDA platform is used by application developers to create applications that run on many generations of GPU architectures, including future GPU architectures yet to be invented. While new versions 5. 6. Compute Capability 19CUDA C++ Programming Guide, Release 12. 5 of the CUDA platform often add native support for a new GPU architecture by supporting the compute capability version of that architecture, new versions of the CUDA platform typically also include software features that are independent of hardware generation. TheTesla andFermi architectures are no longer supported starting with CUDA 7. 0 and CUDA 9. 0, respectively. 20 Chapter 5. Programming Model"

        }
        
        # Code Generation Prompts
        self.prompt_dict = {
            "0": "Write a complete and compilable C++ program that computes the prefix sum (scan operation) of an array of integers. The array should have a size of ARRAY_SIZE_VAR. Use OpenMP to parallelize the computation, ensuring that the program is optimized for multi-threaded execution on a CPU with at least THREAD_NUM_VAR threads. The program should include: 1. Proper inclusion of necessary headers. 2. Initialization of the array with example values. 3. Parallel computation of the prefix sum using OpenMP directives. 4. Output of the resulting prefix sum array. 5. Comments explaining each major step and section of the code. Ensure the code is formatted for readability and follows best practices for C++ and OpenMP programming.",
        
            "1": "Write a complete and compilable C++ program that applies a constant function to each floating-point element of an array. The array should have a size of ARRAY_SIZE. Use OpenMP to parallelize the computation, ensuring that the program is optimized for multi-threaded execution on a CPU with at least VAR_NUM threads. The program should include: 1. Proper inclusion of necessary headers. 2. Initialization of the array with example floating-point values. 3. Definition of the constant function to be applied. 4. Parallel application of the constant function to each array element using OpenMP directives. 5. Output of the resulting array. 6. Comments explaining each major step and section of the code. Ensure the code is formatted for readability and follows best practices for C++ and OpenMP programming.",

            "2": "Write a complete and compilable C++ program that demonstrates dense matrix multiplication for square matrices using OpenMP pragma directives. The matrices should have a size of <MATRIX_SIZE_VAR> x <MATRIX_SIZE_VAR>. Optimize the code for runtime performance on a CPU with at least <THREAD_NUM_VAR> threads. The program should include: 1. Proper inclusion of necessary headers. 2. Generation of random floating-point values for the initial matrices. 3. Parallelized multiplication of the matrices using OpenMP directives. 4. Output of the resulting matrix. 5. Comments explaining each major step and section of the code. 6. Ensure the code is formatted for readability and follows best practices for C++ and OpenMP programming.",

            "3": "Write a complete and compilable C++ program that sums each randomly generated floating-point element in an array using OpenMP pragma directives. The array should have a size of <ARRAY_SIZE_VAR>. Use a for loop for the summation calculation. Optimize the code for runtime performance on a CPU with at least <THREAD_NUM_VAR> threads. The program should include: 1. Proper inclusion of necessary headers. 2. Generation of random floating-point values for the array. 3. Parallelized summation of the array elements using OpenMP directives and a for loop. 4. Output of the total sum. 5. Comments explaining each major step and section of the code. 6. Ensure the code is formatted for readability and follows best practices for C++ and OpenMP programming.",

            "4": "Write a complete and compilable C++ program that sorts an array of randomly generated floating-point values using OpenMP pragma directives. The array should have a size of <ARRAY_SIZE_VAR>. Provide an in-place version of the sorting algorithm. Optimize the code for runtime performance on a CPU with at least <THREAD_NUM_VAR> threads. The program should include: 1. Proper inclusion of necessary headers. 2. Generation of random floating-point values for the array. 3. Parallelized sorting of the array elements using OpenMP directives. 4. Output of the sorted array. 5. Comments explaining each major step and section of the code. Ensure the code is formatted for readability and follows best practices for C++ and OpenMP programming.",

            "5": "Write a complete and compilable C++ program that sorts an array of randomly generated floating-point values using OpenMP pragma directives. The array should have a size of <ARRAY_SIZE_VAR>. Provide an out-of-place version of the sorting algorithm. Optimize the code for runtime performance on a CPU with at least <THREAD_NUM_VAR> threads. The program should include: 1. Proper inclusion of necessary headers. 2. Generation of random floating-point values for the array. 3. Parallelized sorting of the array elements using OpenMP directives and an out-of-place sorting algorithm. 4. Output of the original and sorted arrays. 5. Comments explaining each major step and section of the code. Ensure the code is formatted for readability and follows best practices for C++ and OpenMP programming.",

            "6": "Write a complete and compilable C++ program that searches for a specific element or property within an array using OpenMP pragma directives. The array should have a size of <ARRAY_SIZE_VAR> and be comprised of randomly generated floating-point values. The program should be optimized for multi-threaded execution on a CPU with at least <THREAD_NUM_VAR> threads. The program should include: 1. Proper inclusion of necessary headers. 2. Generation of random floating-point values for the array. 3. Definition of the specific element or property to search for. 4. Parallelized search using OpenMP directives. 5. Output of the search result indicating whether the element or property was found and its position if applicable. 6. Comments explaining each major step and section of the code. Ensure the code is formatted for readability and follows best practices for C++ and OpenMP programming.",

            "7": "Write a complete and compilable C++ program that computes the convex hull of a set of points using OpenMP pragma directives. The set should have a size of <SET_SIZE_VAR> and be comprised of randomly generated floating-point coordinates. Optimize the code for runtime performance on a CPU with at least <THREAD_NUM_VAR> threads. The program should include: 1. Proper inclusion of necessary headers. 2. Generation of random floating-point coordinates for the set of points. 3. Parallelized computation of the convex hull using OpenMP directives. 4. Output of the points that form the convex hull. 5. Comments explaining each major step and section of the code. Ensure the code is formatted for readability and follows best practices for C++ and OpenMP programming.",

            "8": "Write a complete and compilable C++ program that performs a single iteration of a 1D stencil problem, such as the Jacobi method, using OpenMP pragma directives. Optimize the code for multi-threaded execution on a CPU with at least <THREAD_VAR> threads. The program should include: 1. Proper inclusion of necessary headers. 2. Initialization of the 1D array with example values. 3. Parallelized computation for a single iteration of the stencil problem using OpenMP directives. 4. Output of the array after the iteration. 5. Comments explaining each major step and section of the code. Ensure the code is formatted for readability and follows best practices for C++ and OpenMP programming.",

            "9": "Write a complete and compilable C++ program that performs a single iteration of a 2D stencil problem, such as the Jacobi method, using OpenMP pragma directives. Optimize the code for multi-threaded execution on a CPU with at least <THREAD_VAR> threads. The program should include: 1. Proper inclusion of necessary headers. 2. Initialization of the 2D array with example values. 3. Parallelized computation for a single iteration of the stencil problem using OpenMP directives. 4. Output of the array after the iteration. 5. Comments explaining each major step and section of the code. Ensure the code is formatted for readability and follows best practices for C++ and OpenMP programming.",

            "10": "Write a complete and compilable C++ program that parallelizes a graph algorithm, specifically for component counting, using OpenMP pragma directives. Optimize the code for multi-threaded execution on a CPU with at least <THREAD_VAR> threads. The program should include: 1. Proper inclusion of necessary headers. 2. Initialization of the graph with example values. 3. Parallelized component counting algorithm using OpenMP directives. 4. Output of the number of components found in the graph. 5. Comments explaining each major step and section of the code. Ensure the code is formatted for readability and follows best practices for C++ and OpenMP programming.",

            "11": "Write a complete and compilable C++ program that computes the Standard Fourier Transform using OpenMP pragma directives. Randomly generate a data set as needed for the computation. Optimize the code for multi-threaded execution on a CPU with at least <THREAD_VAR> threads. The program should include: 1. Proper inclusion of necessary headers. 2. Generation of a random data set for the Fourier Transform computation. 3. Parallelized computation of the Standard Fourier Transform using OpenMP directives. 4. Output of the Fourier Transform results. 5. Comments explaining each major step and section of the code. Ensure the code is formatted for readability and follows best practices for C++ and OpenMP programming.", 

            "12": "Write a complete and compilable C++ program that computes the Inverse Fourier Transform using OpenMP pragma directives. Randomly generate a data set as needed for the computation. Optimize the code for multi-threaded execution on a CPU with at least <THREAD_NUM_VAR> threads. The program should include: 1. Proper inclusion of necessary headers. 2. Generation of a random data set for the Inverse Fourier Transform computation. 3. Parallelized computation of the Inverse Fourier Transform using OpenMP directives. 4. Output of the Inverse Fourier Transform results. 5. Comments explaining each major step and section of the code. Ensure the code is formatted for readability and follows best practices for C++ and OpenMP programming.",

            "13": "Write a complete and compilable C++ program that bins randomly generated floating-point values in an array based on a specific property of the data (e.g., range bins) using OpenMP pragma directives. The array should have a size of <ARRAY_SIZE_VAR>. Optimize the code for multi-threaded execution on a CPU with at least <THREAD_NUM_VAR> threads. The program should include: 1. Proper inclusion of necessary headers. 2. Generation of random floating-point values for the array. 3. Definition and initialization of the bins based on the specific property. 4. Parallelized binning of the array elements using OpenMP directives. 5. Output of the bin counts or contents. 6. Comments explaining each major step and section of the code. Ensure the code is formatted for readability and follows best practices for C++ and OpenMP programming.",

            "14": "Write a complete and compilable C++ program that parallelizes a vector operation from BLAS Level 1 using OpenMP. Focus on optimizing the sparse operation (e.g., sparse vector addition or dot product) for efficient execution on a multi-threaded CPU. The array should have a size of <ARRAY_SIZE_VAR>. The program should be optimized for multi-threaded execution on a CPU with at least <THREAD_NUM_VAR> threads. The program should include: 1. Proper inclusion of necessary headers. 2. Generation of random floating-point values for the vectors. 3. Parallelized computation of the specified BLAS Level 1 operation using OpenMP directives. 4. Output of the computation results. 5. Comments explaining each major step and section of the code. Ensure the code is formatted for readability and follows best practices for C++ and OpenMP programming.",

            "15": "Write a complete and compilable C++ program that parallelizes a sparse matrix-vector multiplication from BLAS Level 2 using OpenMP. Focus on optimizing the sparse operation for efficient execution on a multi-threaded CPU. The matrix should have dimensions of <MATRIX_SIZE_VAR> x <MATRIX_SIZE_VAR>. The program should be optimized for multi-threaded execution on a CPU with at least <THREAD_NUM_VAR> threads. The program should include: 1. Proper inclusion of necessary headers. 2. Generation of random floating-point values for the sparse matrix and vector. 3. Parallelized computation of the sparse matrix-vector multiplication using OpenMP directives. 4. Output of the computation results. 5. Comments explaining each major step and section of the code. Ensure the code is formatted for readability and follows best practices for C++ and OpenMP programming.",

            "16": "Write a complete and compilable C++ program that parallelizes a sparse matrix-matrix multiplication from BLAS Level 3 using OpenMP. Focus on optimizing the sparse operation for efficient execution on a multi-threaded CPU. The matrices should have dimensions of <MATRIX_DIM> x <MATRIX_DIM>. The program should be optimized for multi-threaded execution on a CPU with at least <THREAD_NUM_VAR> threads. The program should include: 1. Proper inclusion of necessary headers. 2. Generation of random floating-point values for the sparse matrices. 3. Parallelized computation of the sparse matrix-matrix multiplication using OpenMP directives. 4. Output of the computation results. 5. Comments explaining each major step and section of the code. Ensure the code is formatted for readability and follows best practices for C++ and OpenMP programming.",

            "17": "Write a complete and compilable C++ program that parallelizes a dense vector operation from BLAS Level 1 using OpenMP. Focus on optimizing the operation (e.g., vector addition, dot product) for efficient execution on a multi-threaded CPU. The vector should have a size of <ARRAY_SIZE_VAR>. The program should be optimized for multi-threaded execution on a CPU with at least <THREAD_NUM_VAR> threads. The program should include: 1. Proper inclusion of necessary headers. 2. Generation of random floating-point values for the vectors. 3. Parallelized computation of the specified BLAS Level 1 operation using OpenMP directives. 4. Output of the computation results. 5. Comments explaining each major step and section of the code. Ensure the code is formatted for readability and follows best practices for C++ and OpenMP programming.",

            "18": "Write a complete and compilable C++ program that parallelizes a dense matrix-vector multiplication from BLAS Level 2 using OpenMP. Focus on optimizing the operation for efficient execution on a multi-threaded CPU. The matrix should have dimensions of <MATRIX_DIM> x <MATRIX_DIM>. The program should be optimized for multi-threaded execution on a CPU with at least <THREAD_NUM_VAR> threads. The program should include: 1. Proper inclusion of necessary headers. 2. Generation of random floating-point values for the dense matrix and vector. 3. Parallelized computation of the dense matrix-vector multiplication using OpenMP directives. 4. Output of the computation results. 5. Comments explaining each major step and section of the code. Ensure the code is formatted for readability and follows best practices for C++ and OpenMP programming.",

            "19": "Write a complete and compilable C++ program that parallelizes a dense matrix-matrix multiplication from BLAS Level 3 using OpenMP. Focus on optimizing the operation for efficient execution on a multi-threaded CPU. The matrices should have dimensions of <MATRIX_DIM> x <MATRIX_DIM>. The program should be optimized for multi-threaded execution on a CPU with at least <THREAD_NUM_VAR> threads. The program should include: 1. Proper inclusion of necessary headers. 2. Generation of random floating-point values for the dense matrices. 3. Parallelized computation of the dense matrix-matrix multiplication using OpenMP directives. 4. Output of the computation results. 5. Comments explaining each major step and section of the code. Ensure the code is formatted for readability and follows best practices for C++ and OpenMP programming."
            
        }
    
    # GET Methods to grab specific prompt
    def get_system_prompt(self, key):
        return self.system_prompt_dict.get(key, "Key not found")

    def get_codetranslate_prompt(self, key):
        return self.codetranslate_prompt_dict.get(key, "Key not found")
        
    def get_value(self, key):
        return self.prompt_dict.get(key, "Key not found")

    def get_codeknowledge(self, key):
        return self.codeknowledge_dict.get(key, "Key not found")